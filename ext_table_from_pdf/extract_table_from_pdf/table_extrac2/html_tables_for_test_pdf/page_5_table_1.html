<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>[1]</th>
      <th>Mistrallite. URL https://huggingface.co/amazon/MistralLite.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>[2]</td>
      <td>Introducing Qwen-7B: Open foundation and human-aligned models (of the state-of-the-arts).</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>URL https://github.com/QwenLM/Qwen-7B/blob/main/tech_memo.md.</td>
    </tr>
    <tr>
      <td>[3]</td>
      <td>Long-data collections. URL https://huggingface.co/datasets/togethercomputer/</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>Long-Data-Collections.</td>
    </tr>
    <tr>
      <td>[4]</td>
      <td>Z. Azerbayev, E. Ayers, , and B. Piotrowski. Proof-pile, 2022. URL https://github.com/</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>zhangir-azerbayev/proof-pile.</td>
    </tr>
    <tr>
      <td>[5]</td>
      <td>S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy,</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>K. McDonell, J. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang,</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>and S. Weinbach. GPT-NeoX-20B: An open-source autoregressive language model, 2022.</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>arXiv: 2204.06745.</td>
    </tr>
    <tr>
      <td>[6]</td>
      <td>bloc97. NTK-Aware Scaled RoPE allows LLaMA models to have extended</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>(8k+) context size without any fine-tuning and minimal perplexity degradation.,</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>scaled_rope_allows_llama_models_to_have/.</td>
    </tr>
    <tr>
      <td>[7]</td>
      <td>bloc97. Add NTK-Aware interpolation "by parts" correction, 2023. URL https://github</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>com/jquesnelle/scaled-rope/pull/1.</td>
    </tr>
    <tr>
      <td>[8]</td>
      <td>C. Chen. Transformer Inference Arithmetic, 2022. URL https://kipp.ly/blog/</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>transformer-inference-arithmetic/.</td>
    </tr>
    <tr>
      <td>[9]</td>
      <td>S. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>via positional interpolation, 2023. arXiv: 2306.15595.</td>
    </tr>
    <tr>
      <td>[10]</td>
      <td>A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao,</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope,</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat,</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito,</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick,</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee,</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck,</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>J. Dean, S. Petrov, and N. Fiedel. PaLM: Scaling language modeling with pathways, 2022.</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>arXiv: 2204.02311.</td>
    </tr>
    <tr>
      <td>[11]</td>
      <td>P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>you have solved question answering? try ARC, the AI2 Reasoning Challenge, 2018. arXiv:</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>1803.05457.</td>
    </tr>
    <tr>
      <td>[12]</td>
      <td>T. Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.</td>
    </tr>
    <tr>
      <td>NaN</td>
      <td>URL https://github.com/togethercomputer/RedPajama-Data.</td>
    </tr>
    <tr>
      <td>[13]</td>
      <td>T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.</td>
    </tr>
  </tbody>
</table>