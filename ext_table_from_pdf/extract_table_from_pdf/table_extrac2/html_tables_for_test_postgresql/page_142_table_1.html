<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>depend on the specific application, but for most purposes it is adequate to use a predefined set of classes.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>PostgreSQL uses a parser to perform this step. A standard parser is provided, and custom parsers can</td>
    </tr>
    <tr>
      <td>be created for specific needs.</td>
    </tr>
    <tr>
      <td>Converting tokens into lexemes. A lexeme is a string, just like a token, but it has been normalized so that</td>
    </tr>
    <tr>
      <td>different forms of the same word are made alike. For example, normalization almost always includes</td>
    </tr>
    <tr>
      <td>folding upper-case letters to lower-case, and often involves removal of suffixes (such as s or es in</td>
    </tr>
    <tr>
      <td>English). This allows searches to find variant forms of the same word, without tediously entering all the</td>
    </tr>
    <tr>
      <td>possible variants. Also, this step typically eliminates stop words, which are words that are so common</td>
    </tr>
    <tr>
      <td>that they are useless for searching. (In short, then, tokens are raw fragments of the document text, while</td>
    </tr>
    <tr>
      <td>lexemes are words that are believed useful for indexing and searching.) PostgreSQL uses dictionaries</td>
    </tr>
    <tr>
      <td>to perform this step. Various standard dictionaries are provided, and custom ones can be created for</td>
    </tr>
    <tr>
      <td>specific needs.</td>
    </tr>
    <tr>
      <td>Storing preprocessed documents optimized for searching. For example, each document can be repre-</td>
    </tr>
    <tr>
      <td>sented as a sorted array of normalized lexemes. Along with the lexemes it is often desirable to store</td>
    </tr>
  </tbody>
</table>